{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 285 - MLIP -Assignment 1 - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using MNISTtools.load, store the images and labels from the training datasets into two variables, respectively, xtrain and ltrain. What are the shapes of both variables? What is the size of the training dataset? What is the feature dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The shape of xtrain is 784*60000;\n",
    "#### The shape of ltrain is 60000*1;\n",
    "#### The size of training dataset is 60000;\n",
    "#### The feature dimension is 784\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Display the image of index 42 and check that its content corresponds to its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "MNISTtools.show(xtrain[:,42])\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The content of image with index 42 is corresponding to its label which is 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the range of xtrain (minimum and maximum values)? What is the type of xtrain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(np.min(xtrain))\n",
    "print(np.max(xtrain))\n",
    "print(type(xtrain))\n",
    "print(xtrain.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create a function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    return ((x - 127.5)/127.5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Using integer array indexing, complete the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d\n",
    "dtrain = label2onehot(ltrain)\n",
    "\n",
    "print(dtrain.shape)\n",
    "print(dtrain[:,42])\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "print(ltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Complete the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis = 0)  # 0 represents columns, 1 represents rows\n",
    "    # numpy.argmax(axis = 0 or 1) returns the index of the maximum for certain column or row\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(all(ltrain == onehot2label(dtrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    div = np.exp(a - a.max(axis = 0))\n",
    "    return div / div.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial g(a)_i}{\\partial a_i} &= \\frac{exp(a_i)*\\sum_{j=1}^{10}  exp(a_j)-exp(a_i)^2}{\\sum_{j = 1}^{10}exp(a_j)^2} \\\\\n",
    "&= g(a)_i - (g(a)_i)^2 \\\\\n",
    "&= g(a)_i(1 - g(a)_i) \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial g(a)_i}{\\partial a_j} &=\n",
    "\\frac {-exp(a_i)*exp(a_j)}{\\sum_{j=1}^{10}exp(a_j)^2} \\\\\n",
    "&= -g(a_i)*g(a)_j (i\\neq j)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    g = softmax(a)\n",
    "    return g*e-(g*e).sum(axis=0)*g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.953000214778419e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = softmaxp(a, e)\n",
    "diff_approx = (softmax(a+eps*e)-softmax(a))/eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12 For the hidden layers, we will using $ReLU(a)_i = max(a_i,0).$Write two functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    a[(a < 0)] = 0\n",
    "    a[0] = 0\n",
    "    return a\n",
    "def relup(a, e):\n",
    "    ret_a = np.zeros(a.shape)\n",
    "    ret_a[(a> 0)] = 1\n",
    "    return ret_a * e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13 Use the following function to create/initialize your shallow network as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "saved_init_net = copy.deepcopy(netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14 Complete the function forwardprop_shallow to evaluate the prediction of our initial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return y\n",
    "\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15 Complete the function eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25985330248829275 should be around .26\n"
     ]
    }
   ],
   "source": [
    "def eval_loss(y, d):\n",
    "    y = np.log(y)\n",
    "    return -np.sum(d*y) / d.size\n",
    "    \n",
    "print(eval_loss(yinit,dtrain),'should be around .26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1559004  0.16872822 0.04224353 ... 0.17993854 0.13240556 0.11999424]\n",
      " [0.04746604 0.0509615  0.04272257 ... 0.06013684 0.03949688 0.12424358]\n",
      " [0.03298208 0.04016292 0.05336364 ... 0.05245161 0.0390839  0.04730759]\n",
      " ...\n",
      " [0.12266958 0.1342491  0.06788984 ... 0.13541795 0.14171889 0.09457247]\n",
      " [0.04847042 0.0449802  0.03249639 ... 0.04746145 0.03817322 0.05285681]\n",
      " [0.39804475 0.28281255 0.23058769 ... 0.27948465 0.32690785 0.31942444]]\n"
     ]
    }
   ],
   "source": [
    "print(yinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16 Complete the function eval_perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9050833333333334\n"
     ]
    }
   ],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    matrix = np.ones(lbl.shape)\n",
    "    matrix[onehot2label(y) - lbl == 0] = 0\n",
    "    return np.sum(matrix)*1.0/len(lbl)\n",
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17 Complete the following function update_shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma = .05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    \n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    # forward process\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    e = -d/y\n",
    "    d2 = softmaxp(a2, e)\n",
    "    d1 = relup(a1, W2.T.dot(d2))\n",
    "    # Gradient descent\n",
    "    W2 = W2 - gamma*d2.dot(h1.T)\n",
    "    W1 = W1 - gamma*d1.dot(x.T)\n",
    "    b2 = b2 - gamma*d2.sum(axis = 1).reshape(No,1)\n",
    "    b1 = b1 - gamma*d1.sum(axis = 1).reshape(Nh,1)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18 Using update_shallow, complete the function backprop_shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0 loss= 0.24538052976217384 error_rate= 0.90655\n",
      "step=1 loss= 0.21962441164441934 error_rate= 0.8164666666666667\n"
     ]
    }
   ],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma = 0.05):\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        if (t % 1 == 0):\n",
    "            ypred = forwardprop_shallow(x, net)\n",
    "            loss = eval_loss(ypred, d)\n",
    "            acc = eval_perfs(ypred, lbl)\n",
    "            print(\"step=\"+str(t)+\" loss= \"+str(loss)+\" error_rate= \"+str(acc))\n",
    "    return net\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0 loss= 0.24581866846239078 error_rate= 0.9039333333333334\n",
      "step=1 loss= 0.22934417107539315 error_rate= 0.7976666666666666\n",
      "step=2 loss= 0.21526223779383463 error_rate= 0.7977166666666666\n",
      "step=3 loss= 0.206345820186148 error_rate= 0.6813666666666667\n",
      "step=4 loss= 0.19829943509572132 error_rate= 0.6366833333333334\n"
     ]
    }
   ],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma = 0.05):\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        if (t % 1 == 0):\n",
    "            ypred = forwardprop_shallow(x, net)\n",
    "            loss = eval_loss(ypred, d)\n",
    "            acc = eval_perfs(ypred, lbl)\n",
    "            print(\"step=\"+str(t)+\" loss= \"+str(loss)+\" error_rate= \"+str(acc))\n",
    "    return net\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0 loss= 0.24341092534876146 error_rate= 0.8878833333333334\n",
      "step=2 loss= 0.20811322637833207 error_rate= 0.7292166666666666\n",
      "step=4 loss= 0.19343459410366823 error_rate= 0.6218166666666667\n",
      "step=6 loss= 0.1802421581389252 error_rate= 0.53035\n",
      "step=8 loss= 0.1677035273476112 error_rate= 0.4597833333333333\n",
      "step=10 loss= 0.15602479692988838 error_rate= 0.40495\n",
      "step=12 loss= 0.14536584496603014 error_rate= 0.3662\n",
      "step=14 loss= 0.135734278669354 error_rate= 0.33448333333333335\n",
      "step=16 loss= 0.1271009085336038 error_rate= 0.3097\n",
      "step=18 loss= 0.11939161389285223 error_rate= 0.29035\n"
     ]
    }
   ],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma = 0.05):\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        if (t % 2 == 0):\n",
    "            ypred = forwardprop_shallow(x, net)\n",
    "            loss = eval_loss(ypred, d)\n",
    "            acc = eval_perfs(ypred, lbl)\n",
    "            print(\"step=\"+str(t)+\" loss= \"+str(loss)+\" error_rate= \"+str(acc))\n",
    "    return net\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma = 0.05):\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        if (t % 10 == 0):\n",
    "            ypred = forwardprop_shallow(x, net)\n",
    "            loss = eval_loss(ypred, d)\n",
    "            acc = eval_perfs(ypred, lbl)\n",
    "            print(\"step=\"+str(t)+\" loss= \"+str(loss)+\" error_rate= \"+str(acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0 loss= 0.22629537418267046 error_rate= 0.8213833333333334\n",
      "step=10 loss= 0.16152120497819006 error_rate= 0.43771666666666664\n",
      "step=20 loss= 0.11813313965266975 error_rate= 0.30701666666666666\n",
      "step=30 loss= 0.09920943231250402 error_rate= 0.27\n",
      "step=40 loss= 0.08243735291759938 error_rate= 0.22191666666666668\n",
      "step=50 loss= 0.07254176574539453 error_rate= 0.19595\n",
      "step=60 loss= 0.0653521134364137 error_rate= 0.17491666666666666\n",
      "step=70 loss= 0.060159801310028224 error_rate= 0.16141666666666668\n",
      "step=80 loss= 0.05596531220195758 error_rate= 0.14968333333333333\n",
      "step=90 loss= 0.052708483998101 error_rate= 0.14155\n",
      "step=100 loss= 0.05019050723257061 error_rate= 0.13621666666666668\n"
     ]
    }
   ],
   "source": [
    "netinit = init_shallow(Ni, Nh, No)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19 Load the testing dataset into two variables xtest and ltest. What is the size of the testing set? Evaluate the performance of your network on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "xtest,ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "dtest = label2onehot(ltest)\n",
    "print(xtest.shape)\n",
    "print(ltest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.048366963406012965 error_rate= 0.1318\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest, nettrain)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 Using update_shallow, interpret each instruction and complete the function backprop_minibatch_shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    N = x.shape[1]\n",
    "    NB = int((N+B-1)/B)\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        for l in range(0, NB):\n",
    "            #index = np.arange(B*l, min(B*(l+1), N))\n",
    "            index = shuffled_indices[B*l:min(B*(l+1),N)]\n",
    "            net = update_shallow(x[:,index],d[:,index],net,gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        loss = eval_loss(y,d)\n",
    "        acc = eval_perfs(y,lbl)\n",
    "        print(\"epoch= \"+str(t)+\" loss= \"+str(loss)+\" error_rate= \"+str(acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21 Run backprop_minibatch_shallow for 5 epochs. Compare the performance of this new network on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 loss= 0.02929680325229088 error_rate= 0.08556666666666667\n",
      "epoch= 1 loss= 0.023077029228314182 error_rate= 0.0672\n",
      "epoch= 2 loss= 0.019282495350412415 error_rate= 0.055016666666666665\n",
      "epoch= 3 loss= 0.01679772367956063 error_rate= 0.04928333333333333\n",
      "epoch= 4 loss= 0.01458215823822353 error_rate= 0.042633333333333336\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit_minibatch = init_shallow(Ni, Nh, No)\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit_minibatch, 5, B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.014902326272132015 error_rate= 0.0444\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest, netminibatch)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred, ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with non-minibatched method, minibatch is faster and with much lower error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
